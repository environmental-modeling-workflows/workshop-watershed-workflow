{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of troubleshooting for mixed-polyhedral mesh\n",
    "\n",
    "We use Coweeta watershed as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing all the package needed to run the watershed workflow\n",
    "\n",
    "# silence shapely2.0 and crs WKT warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# standard python packages\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "import pandas as pd\n",
    "import copy\n",
    "import shapely\n",
    "\n",
    "# watereshed workflow packages\n",
    "import watershed_workflow\n",
    "import watershed_workflow.source_list\n",
    "import watershed_workflow.ui\n",
    "import watershed_workflow.plot\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.densification\n",
    "import watershed_workflow.condition\n",
    "import watershed_workflow.regions\n",
    "\n",
    "# set the verbosity of logging\n",
    "watershed_workflow.ui.setup_logging(1,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symlink hydrography data to avoid downloading NHDPlus on the fly in this short course.\n",
    "# If you wish to use this notebook more generally, please comment or remove this cell!\n",
    "try: os.mkdir('/home/jovyan/data/hydrography')\n",
    "except FileExistsError: pass\n",
    "\n",
    "try: os.symlink('/home/jovyan/data/hydrography/NHDPlus_H_0601_GDB',\n",
    "               '/home/jovyan/workdir/notebooks/data/hydrography/NHDPlus_H_0601_GDB')\n",
    "except FileExistsError: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed.\n",
    "coweeta_shapefile = '../Coweeta_data/input_data/coweeta_basin.shp'\n",
    "hint = '0601'  # hint: HUC 4 containing this shape.\n",
    "               # This is necessary to avoid downloading all HUCs to search for this shape\n",
    "name = 'Coweeta'\n",
    "modis_name = None\n",
    "\n",
    "figsize = (6,6)\n",
    "figsize_3d = (8,6)\n",
    "\n",
    "# Geomtric parameters tuning the degree of cleaning of the raw data and scales of hydrologic features to be considered\n",
    "simplify = 60 # length scale to target average edge\n",
    "ignore_small_rivers = 2\n",
    "prune_by_area_fraction = 0.01\n",
    "\n",
    "# huc boundary refinement control\n",
    "refine_d0 = 20\n",
    "refine_d1 = 100\n",
    "refine_L0 = 70\n",
    "refine_L1 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.daymet_crs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "def trim_linestring(linestring, fraction, from_start=True):\n",
    "    total_length = linestring.length\n",
    "    trim_length = total_length * fraction\n",
    "\n",
    "    if from_start:\n",
    "        # Trimming from the start\n",
    "        current_length = 0\n",
    "        for i, point in enumerate(linestring.coords[:-1]):\n",
    "            segment = LineString([point, linestring.coords[i + 1]])\n",
    "            current_length += segment.length\n",
    "            if current_length >= trim_length:\n",
    "                return LineString([segment.interpolate(current_length - trim_length)] + linestring.coords[i + 1:])\n",
    "\n",
    "    else:\n",
    "        # Trimming from the end (original method)\n",
    "        trimmed_length = total_length - trim_length\n",
    "        current_length = 0\n",
    "        for i, point in enumerate(linestring.coords[:-1]):\n",
    "            segment = LineString([point, linestring.coords[i + 1]])\n",
    "            current_length += segment.length\n",
    "            if current_length >= trimmed_length:\n",
    "                return LineString(linestring.coords[:i + 1] + [segment.interpolate(trimmed_length - (current_length - segment.length))])\n",
    "\n",
    "    return linestring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources and setup\n",
    "\n",
    "Next we set up the source watershed and coordinate system and all data sources for our mesh.  We will use the CRS that is included in the shapefile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wide range of data sources are available; here we use the defaults except for using NHD Plus for watershed boundaries and hydrography (the default is NHD, which is lower resolution and therefore smaller download sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dictionary of source objects\n",
    "sources = watershed_workflow.source_list.get_default_sources()\n",
    "sources['hydrography'] = watershed_workflow.source_list.hydrography_sources['NHD Plus']\n",
    "sources['HUC'] = watershed_workflow.source_list.huc_sources['NHD Plus']\n",
    "sources['depth to bedrock'] = watershed_workflow.source_list.FileManagerSoilGrids2017()\n",
    "\n",
    "\n",
    "#\n",
    "# This demo uses a few datasets that have been clipped out of larger, national\n",
    "# datasets and are distributed with the code.  This is simply to save download\n",
    "# time for this simple problem and to lower the barrier for trying out\n",
    "# Watershed Workflow.  A more typical workflow would delete these lines (as\n",
    "# these files would not exist for other watersheds).\n",
    "#\n",
    "# The default versions of these download large raster and shapefile files that\n",
    "# are defined over a very large region (globally or the entire US).\n",
    "#\n",
    "# Note we also prepopulate some data for MODIS data as well.\n",
    "#\n",
    "sources['land cover'] = watershed_workflow.source_list.FileManagerRaster('../Coweeta_data/input_data/land_cover/land_cover.tif')\n",
    "sources['geologic structure'] = watershed_workflow.source_list.FileManagerGLHYMPS('../Coweeta_data/input_data/GLHYMPS/GLHYMPS.shp')\n",
    "sources['depth to bedrock'] = watershed_workflow.source_list.FileManagerRaster('../Coweeta_data/input_data/DTB/DTB.tif')\n",
    "watershed_workflow.source_list.log_sources(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Surface Mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hucs from shape\n",
    "_, watershed = watershed_workflow.get_split_form_shapes(coweeta_shapefile, out_crs=crs)\n",
    "\n",
    "# note that watershed is now a SplitHUCs object, as described above\n",
    "print(watershed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Rivers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipympl\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle files of already created rivers\n",
    "import pickle\n",
    "with open(\"./data/pickled_data/coweeta_rivers_pickle\", 'rb') as handle:\n",
    "    rivers = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at rivers and watershed boundary\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = watershed_workflow.plot.get_ax(crs, fig, window=[0.05,0.1,0.9,0.8])\n",
    "watershed_workflow.plot.hucs(watershed, crs, 'k', ax)\n",
    "watershed_workflow.plot.rivers(rivers, crs, 'b', ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify and Redensify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the originals\n",
    "rivers_orig=[river.deepcopy() for river in rivers]\n",
    "watershed_orig=copy.deepcopy(watershed)\n",
    "\n",
    "# simplifying\n",
    "rivers = watershed_workflow.simplify(watershed, rivers, simplify_hucs=simplify, simplify_rivers=simplify,\n",
    "                            merge_tol=0.7*simplify, snap_tol=0.5*simplify, cut_intersections=True)\n",
    "\n",
    "# for plotting purpose only\n",
    "rivers_simplified=[river.deepcopy() for river in rivers]\n",
    "watershed_simplified=copy.deepcopy(watershed)\n",
    "\n",
    "print('number of reaches in original', len(rivers_orig[0]), 'number of reaches in simplified', len(rivers[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is because a reach has two intersections with the watershed boundary, a case of inconsistent data of mapped flowlines for river and watershed boundary. As a remedy, we could either trim the river or move the river or watershed boundary segment. \n",
    "\n",
    "Here we will trim the river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert to original river\n",
    "rivers =[river.deepcopy() for river in rivers_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot\n",
    "\n",
    "# fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "# watershed_workflow.plot.hucs(watershed, crs, 'k', ax)\n",
    "\n",
    "# reaches = [r.getSegment() for river in rivers for r in river.preOrder()]\n",
    "# colors = watershed_workflow.colors.enumerated_colors(len(reaches))\n",
    "# lc = watershed_workflow.plot.shplys(reaches, crs, colors, ax=ax, marker='x', picker=True)\n",
    "\n",
    "# labeler = watershed_workflow.plot.Labeler(ax, lc, reaches, format=\"ID: {ID}, HY: {HydrologicSequence}, DIV: {DivergenceCode}, AREA: {ContributingAreaSqKm}\")\n",
    "\n",
    "# def onpick1(event):\n",
    "#     labeler.update(event)\n",
    "# fig.canvas.mpl_connect('pick_event', onpick1)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the problematic node using either node ID (if the dataset is from NHDPlus, this is NHD_ID). We also have widgets that display node id on hovering over nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the problematic node\n",
    "river= rivers[0]\n",
    "node_id = '25000400108019'\n",
    "node = river.getNode(node_id)\n",
    "node.segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_reach_segment = trim_linestring(node.segment, 0.25, from_start=True) # Trims from start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a loot at trimmed node\n",
    "# quick look at rivers and watershed boundary\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = watershed_workflow.plot.get_ax(crs, fig, window=[0.05,0.1,0.9,0.8])\n",
    "watershed_workflow.plot.hucs(watershed, crs, 'k', ax)\n",
    "watershed_workflow.plot.rivers(rivers, crs, 'b', ax)\n",
    "watershed_workflow.plot.shply(trimmed_reach_segment, crs, 'r', ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the edited reach segment back to the node\n",
    "node.segment = trimmed_reach_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at rivers and watershed boundary after editing river\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = watershed_workflow.plot.get_ax(crs, fig, window=[0.05,0.1,0.9,0.8])\n",
    "watershed_workflow.plot.hucs(watershed, crs, 'k', ax)\n",
    "watershed_workflow.plot.rivers(rivers, crs, 'b', ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the originals\n",
    "rivers_orig=[river.deepcopy() for river in rivers]\n",
    "watershed_orig=copy.deepcopy(watershed)\n",
    "\n",
    "# simplifying\n",
    "rivers = watershed_workflow.simplify(watershed, rivers, simplify_hucs=simplify, simplify_rivers=simplify,\n",
    "                            merge_tol=0.7*simplify, snap_tol=0.5*simplify, cut_intersections=True)\n",
    "\n",
    "# for plotting purpose only\n",
    "rivers_simplified=[river.deepcopy() for river in rivers]\n",
    "watershed_simplified=copy.deepcopy(watershed)\n",
    "\n",
    "print('number of reaches in original', len(rivers_orig[0]), 'number of reaches in simplified', len(rivers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what changed between original and simplified river\n",
    "\n",
    "fig, axs = plt.subplots(1,2,subplot_kw={'projection':watershed_workflow.crs.to_cartopy(crs)}, figsize=(10,10))\n",
    "\n",
    "axs[0].plot(watershed_orig.exterior().exterior.xy[0], watershed_orig.exterior().exterior.xy[1], 'k-x')\n",
    "axs[0].set_title('original river network and hucs',fontsize=16)\n",
    "axs[1].plot(watershed.exterior().exterior.xy[0], watershed.exterior().exterior.xy[1], 'k-x')\n",
    "axs[1].set_title('after simplify and prune',fontsize=16)\n",
    "\n",
    "for river in rivers_orig:\n",
    "    for node in river.preOrder():\n",
    "        x,y=node.segment.xy\n",
    "        axs[0].plot(x,y,'-o',markersize=5)\n",
    "\n",
    "for river in rivers_simplified:\n",
    "    for node in river.preOrder():\n",
    "        x,y=node.segment.xy\n",
    "        axs[1].plot(x,y,'-o',markersize=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watershed_workflow-2023-sep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
